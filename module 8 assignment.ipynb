{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# THEORY\n",
        "\n",
        "### 1. What is a Support Vector Machine (SVM)\n",
        "\n",
        "**Answer:**  \n",
        "\n",
        "SVM is a supervised learning algorithm used for classification and regression tasks.  \n",
        "\n",
        "It works by finding the best hyperplane that separates data points of different classes with the maximum margin.  \n",
        "\n",
        "\n",
        "### 2. What is the difference between Hard Margin and Soft Margin SVM\n",
        "\n",
        "**Answer:**  \n",
        "\n",
        "- **Hard Margin SVM** assumes that the data is linearly separable and doesn't allow any misclassification.  \n",
        "\n",
        "- **Soft Margin SVM** allows some misclassifications and introduces slack variables to handle noisy or non-separable data.  \n",
        "\n",
        "\n",
        "### 3. What is the mathematical intuition behind SVM\n",
        "\n",
        "**Answer:**  \n",
        "\n",
        "SVM tries to maximize the margin between two classes by solving an optimization problem:  \n",
        "\n",
        "Minimize \\( \\frac{1}{2} \\|w\\|^2 \\), subject to the constraint that all data points are correctly classified with a margin.  \n",
        "\n",
        "In soft margin, slack variables \\( \\xi_i \\) and penalty term \\( C \\sum \\xi_i \\) are added.  \n",
        "\n",
        "\n",
        "### 4. What is the role of Lagrange Multipliers in SVM\n",
        "\n",
        "**Answer:**  \n",
        "\n",
        "Lagrange Multipliers help convert the constrained optimization problem into a form that can be solved using dual formulation.  \n",
        "\n",
        "They are used to derive the support vectors and the decision boundary in SVM.  \n",
        "\n",
        "\n",
        "### 5. What are Support Vectors in SVM\n",
        "\n",
        "**Answer:**  \n",
        "\n",
        "Support Vectors are the data points that lie closest to the decision boundary.  \n",
        "\n",
        "They directly influence the position and orientation of the separating hyperplane.  \n",
        "\n",
        "\n",
        "### 6. What is a Support Vector Classifier (SVC)\n",
        "\n",
        "**Answer:**  \n",
        "\n",
        "SVC is the implementation of SVM for classification problems.  \n",
        "\n",
        "It uses different kernels to handle linear and non-linear classification tasks.  \n",
        "\n",
        "\n",
        "### 7. What is a Support Vector Regressor (SVR)\n",
        "\n",
        "**Answer:**  \n",
        "\n",
        "SVR is the regression version of SVM.  \n",
        "\n",
        "Instead of maximizing the margin for classification, SVR tries to fit a function within an Œµ-tube around the target values while minimizing model complexity.  \n",
        "\n",
        "\n",
        "### 8. What is the Kernel Trick in SVM\n",
        "\n",
        "**Answer:**  \n",
        "\n",
        "The kernel trick allows SVM to operate in a high-dimensional feature space without explicitly transforming the data.  \n",
        "\n",
        "It uses kernel functions to compute dot products in the transformed space efficiently.  \n",
        "\n",
        "\n",
        "### 9. Compare Linear Kernel, Polynomial Kernel, and RBF Kernel\n",
        "\n",
        "**Answer:**  \n",
        "\n",
        "- **Linear Kernel:** Used for linearly separable data. Simple and fast.  \n",
        "\n",
        "- **Polynomial Kernel:** Captures more complex patterns with polynomial degrees.  \n",
        "\n",
        "- **RBF Kernel:** Handles highly non-linear data. It maps data into infinite-dimensional space.  \n",
        "\n",
        "\n",
        "### 10. What is the effect of the C parameter in SVM\n",
        "\n",
        "**Answer:**  \n",
        "\n",
        "C controls the trade-off between maximizing the margin and minimizing classification error.  \n",
        "\n",
        "- High C ‚Üí less tolerant of misclassifications (hard margin behavior).  \n",
        "- Low C ‚Üí more tolerant of misclassifications (soft margin behavior).  \n",
        "\n",
        "\n",
        "### 11. What is the role of the Gamma parameter in RBF Kernel SVM\n",
        "\n",
        "**Answer:**  \n",
        "\n",
        "Gamma defines the influence of a single training example.  \n",
        "\n",
        "- High gamma ‚Üí closer fitting to training data (risk of overfitting).  \n",
        "- Low gamma ‚Üí smoother decision boundary (more generalization).  \n",
        "\n",
        "\n",
        "### 12. What is the Na√Øve Bayes classifier, and why is it called \"Na√Øve\"\n",
        "\n",
        "**Answer:**  \n",
        "\n",
        "Na√Øve Bayes is a probabilistic classifier based on Bayes' Theorem.  \n",
        "\n",
        "It is called \"na√Øve\" because it assumes that all features are conditionally independent given the class label.  \n",
        "\n",
        "\n",
        "### 13. What is Bayes‚Äô Theorem\n",
        "\n",
        "**Answer:**  \n",
        "\n",
        "\\[\n",
        "P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n",
        "\\]  \n",
        "\n",
        "It describes the probability of event A occurring given that event B is true.  \n",
        "\n",
        "\n",
        "### 14. Explain the differences between Gaussian Na√Øve Bayes, Multinomial Na√Øve Bayes, and Bernoulli Na√Øve Bayes\n",
        "\n",
        "**Answer:**  \n",
        "\n",
        "- **Gaussian NB:** Assumes features follow a normal distribution. Used for continuous data.  \n",
        "\n",
        "- **Multinomial NB:** Used for discrete counts (e.g., word counts in text).  \n",
        "\n",
        "- **Bernoulli NB:** Assumes binary features (presence or absence of words).  \n",
        "\n",
        "\n",
        "### 15. When should you use Gaussian Na√Øve Bayes over other variants\n",
        "\n",
        "**Answer:**  \n",
        "\n",
        "Use Gaussian NB when your input features are continuous and roughly follow a normal distribution.  \n",
        "\n",
        "\n",
        "### 16. What are the key assumptions made by Na√Øve Bayes\n",
        "\n",
        "**Answer:**  \n",
        "\n",
        "- All features are conditionally independent given the class label.  \n",
        "\n",
        "- All features contribute equally and independently to the outcome.  \n",
        "\n",
        "\n",
        "### 17. What are the advantages and disadvantages of Na√Øve Bayes\n",
        "\n",
        "**Answer:**  \n",
        "\n",
        "**Advantages:**  \n",
        "\n",
        "- Fast and efficient  \n",
        "- Works well with high-dimensional data  \n",
        "- Performs well on text classification  \n",
        "\n",
        "**Disadvantages:**  \n",
        "\n",
        "- Assumes feature independence  \n",
        "- Doesn't work well with correlated features  \n",
        "\n",
        "\n",
        "### 18. Why is Na√Øve Bayes a good choice for text classification\n",
        "\n",
        "**Answer:**  \n",
        "\n",
        "Text data often has many features (words), and Na√Øve Bayes handles high-dimensional spaces well.  \n",
        "\n",
        "It‚Äôs fast, works well with sparse data, and gives good performance in practice.  \n",
        "\n",
        "\n",
        "### 19. Compare SVM and Na√Øve Bayes for classification tasks\n",
        "\n",
        "**Answer:**  \n",
        "\n",
        "- **SVM:** More powerful and flexible, handles complex boundaries well, slower training.  \n",
        "\n",
        "- **Na√Øve Bayes:** Simpler, faster, and works surprisingly well for text.  \n",
        "\n",
        "- Use NB when speed and simplicity matter; use SVM for more complex tasks.  \n",
        "\n",
        "\n",
        "### 20. How does Laplace Smoothing help in Na√Øve Bayes\n",
        "\n",
        "**Answer:**  \n",
        "\n",
        "Laplace Smoothing (add-1 smoothing) prevents zero probabilities for words that are not seen in the training data.  \n",
        "\n",
        "It ensures that the model does not completely discard unseen features during prediction.  \n"
      ],
      "metadata": {
        "id": "5lLxUvq7Tg94"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PRACTICAL"
      ],
      "metadata": {
        "id": "7paW3Ta4To-O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ü§ñ SVM and Na√Øve Bayes Practical Questions (with Descriptive Answers)\n",
        "\n",
        "### 21. Write a Python program to train an SVM Classifier on the Iris dataset and evaluate accuracy\n",
        "\n",
        "**Answer:**  \n",
        "\n",
        "Use `sklearn.datasets.load_iris`, `SVC`, and `train_test_split`.  \n",
        "\n",
        "Train the model, make predictions, and evaluate using `accuracy_score`.  \n",
        "\n",
        "\n",
        "### 22. Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies\n",
        "\n",
        "**Answer:**  \n",
        "\n",
        "Use `SVC(kernel='linear')` and `SVC(kernel='rbf')`.  \n",
        "\n",
        "Train both models and compare `accuracy_score` on test data.  \n",
        "\n",
        "\n",
        "### 23. Write a Python program to train an SVM Regressor (SVR) on a housing dataset and evaluate it using Mean Squared Error (MSE)\n",
        "\n",
        "**Answer:**  \n",
        "\n",
        "Use `SVR()` from `sklearn.svm`, train on housing data, and evaluate using `mean_squared_error`.  \n",
        "\n",
        "\n",
        "### 24. Write a Python program to train an SVM Classifier with a Polynomial Kernel and visualize the decision boundary\n",
        "\n",
        "**Answer:**  \n",
        "\n",
        "Use `SVC(kernel='poly')` and `matplotlib` or `seaborn` for 2D decision boundary plotting.  \n",
        "\n",
        "\n",
        "### 25. Write a Python program to train a Gaussian Na√Øve Bayes classifier on the Breast Cancer dataset and evaluate accuracy\n",
        "\n",
        "**Answer:**  \n",
        "\n",
        "Use `GaussianNB()` from `sklearn.naive_bayes` with `load_breast_cancer()` dataset.  \n",
        "\n",
        "Evaluate using `accuracy_score`.  \n",
        "\n",
        "\n",
        "### 26. Write a Python program to train a Multinomial Na√Øve Bayes classifier for text classification using the 20 Newsgroups dataset\n",
        "\n",
        "**Answer:**  \n",
        "\n",
        "Use `MultinomialNB()` and `fetch_20newsgroups_vectorized()`.  \n",
        "\n",
        "Train and evaluate the model using accuracy or F1 score.  \n",
        "\n",
        "\n",
        "### 27. Write a Python program to train an SVM Classifier with different C values and compare the decision boundaries visually\n",
        "\n",
        "**Answer:**  \n",
        "\n",
        "Loop through several values of `C` in `SVC()` and plot decision boundaries using `matplotlib`.  \n",
        "\n",
        "\n",
        "### 28. Write a Python program to train a Bernoulli Na√Øve Bayes classifier for binary classification on a dataset with binary features\n",
        "\n",
        "**Answer:**  \n",
        "\n",
        "Use `BernoulliNB()` on binary datasets such as text data with presence/absence of words.  \n",
        "\n",
        "\n",
        "### 29. Write a Python program to apply feature scaling before training an SVM model and compare results with unscaled data\n",
        "\n",
        "**Answer:**  \n",
        "\n",
        "Use `StandardScaler()` to scale features before training `SVC()`.  \n",
        "\n",
        "Compare model accuracy with and without scaling.  \n",
        "\n",
        "\n",
        "### 30. Write a Python program to train a Gaussian Na√Øve Bayes model and compare the predictions before and after Laplace Smoothing\n",
        "\n",
        "**Answer:**  \n",
        "\n",
        "Compare predictions with and without adding small constant to likelihoods (Laplace Smoothing).  \n",
        "\n",
        "\n",
        "### 31. Write a Python program to train an SVM Classifier and use GridSearchCV to tune the hyperparameters (C, gamma, kernel)\n",
        "\n",
        "**Answer:**  \n",
        "\n",
        "Use `GridSearchCV()` with a parameter grid of C, gamma, and kernel for tuning `SVC`.  \n",
        "\n",
        "\n",
        "### 32. Write a Python program to train an SVM Classifier on an imbalanced dataset and apply class weighting and check if it improves accuracy\n",
        "\n",
        "**Answer:**  \n",
        "\n",
        "Set `class_weight='balanced'` in `SVC()` and evaluate accuracy before and after.  \n",
        "\n",
        "\n",
        "### 33. Write a Python program to implement a Na√Øve Bayes classifier for spam detection using email data\n",
        "\n",
        "**Answer:**  \n",
        "\n",
        "Use `MultinomialNB()` with a preprocessed email dataset (like SMS Spam Collection).  \n",
        "\n",
        "Evaluate using accuracy or F1 score.  \n",
        "\n",
        "\n",
        "### 34. Write a Python program to train an SVM Classifier and a Na√Øve Bayes Classifier on the same dataset and compare their accuracy\n",
        "\n",
        "**Answer:**  \n",
        "\n",
        "Train both `SVC()` and `MultinomialNB()` or `GaussianNB()` on the same train-test split.  \n",
        "\n",
        "Compare their `accuracy_score`.  \n",
        "\n",
        "\n",
        "### 35. Write a Python program to perform feature selection before training a Na√Øve Bayes classifier and compare results\n",
        "\n",
        "**Answer:**  \n",
        "\n",
        "Use `SelectKBest` or `chi2` for feature selection before `MultinomialNB()` or `GaussianNB()`.  \n",
        "\n",
        "Compare accuracy with and without selection.  \n",
        "\n",
        "\n",
        "### 36. Write a Python program to train an SVM Classifier using One-vs-Rest (OvR) and One-vs-One (OvO) strategies on the Wine dataset and compare their accuracy\n",
        "\n",
        "**Answer:**  \n",
        "\n",
        "Use `OneVsRestClassifier(SVC())` and `OneVsOneClassifier(SVC())` and compare accuracies.  \n",
        "\n",
        "\n",
        "### 37. Write a Python program to train an SVM Classifier using Linear, Polynomial, and RBF kernels on the Breast Cancer dataset and compare their accuracy\n",
        "\n",
        "**Answer:**  \n",
        "\n",
        "Train three `SVC()` models with `kernel='linear'`, `kernel='poly'`, and `kernel='rbf'`.  \n",
        "\n",
        "Compare model accuracies.  \n",
        "\n",
        "\n",
        "### 38. Write a Python program to train an SVM Classifier using Stratified K-Fold Cross-Validation and compute the average accuracy\n",
        "\n",
        "**Answer:**  \n",
        "\n",
        "Use `StratifiedKFold` with `cross_val_score(SVC())` to compute mean accuracy.  \n",
        "\n",
        "\n",
        "### 39. Write a Python program to train a Na√Øve Bayes classifier using different prior probabilities and compare performance\n",
        "\n",
        "**Answer:**  \n",
        "\n",
        "Use the `priors` parameter in `GaussianNB()` and compare accuracy or log loss.  \n",
        "\n",
        "\n",
        "### 40. Write a Python program to perform Recursive Feature Elimination (RFE) before training an SVM Classifier and compare accuracy\n",
        "\n",
        "**Answer:**  \n",
        "\n",
        "Use `RFE(estimator=SVC(kernel='linear'))` to select features before training.  \n",
        "\n",
        "Compare model accuracy with full vs selected features.  \n",
        "\n",
        "\n",
        "### 41. Write a Python program to train an SVM Classifier and evaluate its performance using Precision, Recall, and F1-Score instead of accuracy\n",
        "\n",
        "**Answer:**  \n",
        "\n",
        "Use `precision_score`, `recall_score`, and `f1_score` from `sklearn.metrics` to evaluate `SVC`.  \n",
        "\n",
        "\n",
        "### 42. Write a Python program to train a Na√Øve Bayes Classifier and evaluate its performance using Log Loss (Cross-Entropy Loss)\n",
        "\n",
        "**Answer:**  \n",
        "\n",
        "Use `log_loss()` from `sklearn.metrics` on probabilities from `predict_proba()` of NB model.  \n",
        "\n",
        "\n",
        "### 43. Write a Python program to train an SVM Classifier and visualize the Confusion Matrix using seaborn\n",
        "\n",
        "**Answer:**  \n",
        "\n",
        "Use `confusion_matrix()` and `seaborn.heatmap()` after training `SVC()` on test data.  \n",
        "\n",
        "\n",
        "### 44. Write a Python program to train an SVM Regressor (SVR) and evaluate its performance using Mean Absolute Error (MAE) instead of MSE\n",
        "\n",
        "**Answer:**  \n",
        "\n",
        "Train `SVR()` and evaluate using `mean_absolute_error` from `sklearn.metrics`.  \n",
        "\n",
        "\n",
        "### 45. Write a Python program to train a Na√Øve Bayes classifier and evaluate its performance using the ROC-AUC score\n",
        "\n",
        "**Answer:**  \n",
        "\n",
        "Use `roc_auc_score()` with `predict_proba()` or `decision_function()` from `NB` model.  \n",
        "\n",
        "\n",
        "### 46. Write a Python program to train an SVM Classifier and visualize the Precision-Recall Curve\n",
        "\n",
        "**Answer:**  \n",
        "\n",
        "Use `precision_recall_curve()` and `matplotlib.pyplot.plot()` to visualize the curve for `SVC`.  \n"
      ],
      "metadata": {
        "id": "FxzCqbM4T-Z2"
      }
    }
  ]
}